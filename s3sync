#!/bin/bash

# kill running process and write new pid
test -f ~/.s3sync-pid && kill -9 $(cat ~/.s3sync-pid) 2>/dev/null
echo $$ > ~/.s3sync-pid

#config
BUCKET=redbackup
syncdir=~/.s3synctree

# set IFS
oldifs=$IFS
IFS=$'\n'

# get all the timestamps we need
touch ~/.s3sync-synced-stamp
read synced < ~/.s3sync-synced-stamp
today=`date '+%Y%m%d'`
day=$(date +%A)
day_num=$(date +%d)

# determine type
if [ $day_num == 1 ]; then
	synctype=month
elif [ $day != "Saturday" ]; then
    synctype=daily
else 
	synctype=weekly
fi

# make directory structure based on symlinks. This should be very fast.
dirlist=$(cat dirlist.conf | sort)
rm -rf $syncdir && mkdir $syncdir

for d in $dirlist
do
    bdir=$(dirname "$d")
    dir=$(basename "$d")
    sdir_base=$syncdir$bdir
    sdir_target=$syncdir$bdir/$dir
    
    if test ! -d "$sdir_target"
    then
        printf "($now) Creating \"%s\"\n" $sdir_target
        mkdir -p "$sdir_base"
        ln -s $d $sdir_target
    else
        printf "($now) Skipping \"$sdir_target\", already exists.\n"
    fi
done

# Sync data with cloud
if [ "$synced" == "$today" ]; then
    # If it is already done, we do nothing
    echo "Daily sync is already done."
else
    # Sync in case we stil have to sync today
    echo "Syncing..."
    
    # Sync and append output to output file
    read result < <(s3cmd sync $syncdir/ s3://$BUCKET/$synctype/ --follow-symlinks --delete-removed 2>&1)
    
    # And check if we are done, and ff so,
    # set a new synced.stamp so the next round will not sync.
    test "$result" = "" && echo $today > ~/.s3sync-synced-stamp
fi

# clean pid file
rm ~/.s3sync-pid
